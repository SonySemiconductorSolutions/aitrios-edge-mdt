{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8194007-6ea7-4e00-8931-a37ca2d0dd20",
   "metadata": {
    "id": "f8194007-6ea7-4e00-8931-a37ca2d0dd20"
   },
   "source": [
    "# Post-Training Quantization + Conversion Example of MobileNetV2 Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e6d6d-4980-4d66-beed-9ff5a494acf9",
   "metadata": {
    "id": "930e6d6d-4980-4d66-beed-9ff5a494acf9"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699be4fd-d382-4eec-9d3f-e2e85cfb1762",
   "metadata": {
    "id": "699be4fd-d382-4eec-9d3f-e2e85cfb1762"
   },
   "source": [
    "This tutorial demonstrates how do Post Training Quantization using the [**Model Compression Toolkit (MCT)**](https://github.com/sony/model_optimization) and how to convert the resulting model to a format suitable to load to IMX500 using the [**IMX500-converter**](https://github.com/ssi-dnn/imx500-converter) . \n",
    "\n",
    "this example is not intended to demonstrate evaluating MCT PTQ performence - for those steps see - [*MCT PTQ Keras Tutorial*](https://github.com/sony/model_optimization/blob/main/tutorials/notebooks/imx500_notebooks/keras/example_keras_mobilenetv2_for_imx500.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85199e25-c587-41b1-aaf5-e1d23ce97ca1",
   "metadata": {
    "id": "85199e25-c587-41b1-aaf5-e1d23ce97ca1"
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0e9543-d356-412f-acf1-c2ecad553e06",
   "metadata": {
    "id": "9c0e9543-d356-412f-acf1-c2ecad553e06"
   },
   "source": [
    "In this tutorial we cover the following subjects:\n",
    "\n",
    "1. Post-Training Quantization using MCT.\n",
    "2. Converting the model to a IMX500 suitable representation using IMX500-Converter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04228b7c-00f1-4ded-bead-722e2a4e89a0",
   "metadata": {
    "tags": [],
    "id": "04228b7c-00f1-4ded-bead-722e2a4e89a0"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2657cf1a-654d-45a6-b877-8bf42fc26d0d",
   "metadata": {
    "id": "2657cf1a-654d-45a6-b877-8bf42fc26d0d"
   },
   "source": [
    "Install and import the relevant packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324685b9-5dcc-4d22-80f4-dec9a93d3324",
   "metadata": {
    "tags": [],
    "id": "324685b9-5dcc-4d22-80f4-dec9a93d3324"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from importlib import util\n",
    "\n",
    "\n",
    "TF_VER = os.getenv('TF_VER', '2.15.1')\n",
    "IMX500_AI_TOOLCHAIN_VER = os.getenv('IMX500_AI_TOOLCHAIN_VERSION', '0.0.0.dev0')\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except ImportError:\n",
    "    print(f\"Installing TensorFlow {TF_VER}\")\n",
    "    !pip install tensorflow=={TF_VER}\n",
    "\n",
    "if not util.find_spec('imx500_ai_toolchain') or not util.find_spec(\"uni.tensorflow\"):\n",
    "    print(f\"Installing imx500_ai_toolchain {IMX500_AI_TOOLCHAIN_VER}\")\n",
    "    !pip install imx500_ai_toolchain[tf]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f0acc8-281c-4bca-b0b9-3d7677105f19",
   "metadata": {
    "id": "b3f0acc8-281c-4bca-b0b9-3d7677105f19"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import model_compression_toolkit as mct\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7fed0d-cfc8-41ee-adf1-22a98110397b",
   "metadata": {
    "id": "0c7fed0d-cfc8-41ee-adf1-22a98110397b"
   },
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download ImageNet dataset with only the validation split.\n",
    "\n",
    "**Note** that for demonstration purposes we use the validation set for the model quantization routines. Usually, a subset of the training dataset is used, but loading it is a heavy procedure that is unnecessary for the sake of this demonstration.\n",
    "\n",
    "This step may take several minutes..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aecde59e4c37b1da"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IMAGENET_DIR = os.getenv('IMAGENET_DIR', './imagenet')\n",
    "if not os.path.isdir(IMAGENET_DIR):\n",
    "    !mkdir imagenet\n",
    "    !wget https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz\n",
    "    !mv ILSVRC2012_devkit_t12.tar.gz imagenet/\n",
    "    !wget https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar\n",
    "    !mv ILSVRC2012_img_val.tar imagenet/"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c18b26e293b085e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract ImageNet validation dataset using using 'prepare_imagenet.sh' script"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e48ff22f70ce997e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if not os.path.isdir(os.path.join(IMAGENET_DIR, 'val')):\n",
    "    import subprocess\n",
    "    !git clone https://github.com/sony/model_optimization.git temp_mct && mv temp_mct/tutorials . && \\rm -rf temp_mct\n",
    "    !chmod +x tutorials/resources/scripts/prepare_imagenet.sh\n",
    "    subprocess.run(['tutorials/resources/scripts/prepare_imagenet.sh'])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5a6fb997c54aa3fb"
  },
  {
   "cell_type": "markdown",
   "id": "028112db-3143-4fcb-96ae-e639e6476c31",
   "metadata": {
    "id": "028112db-3143-4fcb-96ae-e639e6476c31"
   },
   "source": [
    "Define the required preprocessing method for the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed56f505-97ff-4acb-8ad8-ef09c53e9d57",
   "metadata": {
    "id": "ed56f505-97ff-4acb-8ad8-ef09c53e9d57"
   },
   "outputs": [],
   "source": [
    "def imagenet_preprocess_input(images, labels):\n",
    "    return tf.keras.applications.mobilenet_v2.preprocess_input(images), labels"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Representative dataset construction\n",
    "We show how to create a generator for the representative dataset, which is required for post-training quantization.\n",
    "\n",
    "The representative dataset is used for collecting statistics on the inference outputs of all layers in the model.\n",
    " \n",
    "In order to decide on the size of the representative dataset, we configure the batch size and the number of calibration iterations.\n",
    "This gives us the total number of samples that will be used during PTQ (batch_size x n_iter).\n",
    "In this example we set `batch_size = 50` and `n_iter = 10`, resulting in a total of 500 representative images.\n",
    "\n",
    "Please ensure that the dataset path has been set correctly."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcbb3eecae5346a9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0408f624-ab68-4989-95f8-f9d327882840",
   "metadata": {
    "id": "0408f624-ab68-4989-95f8-f9d327882840"
   },
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "\n",
    "REPRESENTATIVE_DATASET_FOLDER = os.path.join(IMAGENET_DIR, 'val')\n",
    "BATCH_SIZE = 50\n",
    "n_iter=10\n",
    "\n",
    "# Create representative dataset generator\n",
    "def get_representative_dataset() -> Generator:\n",
    "    \"\"\"A function that loads the dataset and returns a representative dataset generator.\n",
    "\n",
    "    Returns:\n",
    "        Generator: A generator yielding batches of preprocessed images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the dataset from folder\n",
    "    print('loading dataset, this may take few minutes ...')    \n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory=REPRESENTATIVE_DATASET_FOLDER,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        image_size=[224, 224],\n",
    "        shuffle=True,\n",
    "        crop_to_aspect_ratio=True,\n",
    "        interpolation='bilinear')  \n",
    "    # Preprocess the data\n",
    "    dataset = dataset.map(lambda x, y: (imagenet_preprocess_input(x, y)))\n",
    "\n",
    "    def representative_dataset() -> Generator:\n",
    "        \"\"\"A generator function that yields batch of preprocessed images.\n",
    "\n",
    "        Yields:\n",
    "            A batch of preprocessed images.\n",
    "        \"\"\"\n",
    "        for _ in range(n_iter):\n",
    "            yield dataset.take(1).get_single_element()[0].numpy()\n",
    "\n",
    "    return representative_dataset\n",
    "\n",
    "# Create a representative dataset generator\n",
    "representative_dataset_gen = get_representative_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e9ba6-2954-4506-ad5c-0da273701ba5",
   "metadata": {
    "id": "4a1e9ba6-2954-4506-ad5c-0da273701ba5"
   },
   "source": [
    "## Model Post-Training quantization using MCT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55edbb99-ab2f-4dde-aa74-4ddee61b2615",
   "metadata": {
    "id": "55edbb99-ab2f-4dde-aa74-4ddee61b2615"
   },
   "source": [
    "This is the main part in which we quantize our model.\n",
    "\n",
    "First, we load a pre-trained MobileNetV2 model from Keras, in 32-bits floating-point precision format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cac59f-ec5e-41ca-b673-96220924a47c",
   "metadata": {
    "id": "80cac59f-ec5e-41ca-b673-96220924a47c"
   },
   "outputs": [],
   "source": [
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "float_model = MobileNetV2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8b486a-ca39-45d9-8699-f7116b0414c9",
   "metadata": {
    "id": "8a8b486a-ca39-45d9-8699-f7116b0414c9"
   },
   "source": [
    "Next, we need to define a `TargetPlatformCapability` object, representing the HW specifications on which we wish to eventually deploy our quantized model.\n",
    "\n",
    "In addition, we need to define the Quantization Configuration for our PTQ routine.\n",
    "\n",
    "Here, we demonstrate how to define a quantization configuration with several key argument that can be controlled by the user.\n",
    "**Note** that you can skip this part if you prefer to use the default quantization settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from model_compression_toolkit.core import QuantizationErrorMethod\n",
    "\n",
    "# Specify the IMX500-v1 target platform capability (TPC) \n",
    "tpc = mct.get_target_platform_capabilities(\"tensorflow\", 'imx500', target_platform_version='v1')\n",
    "\n",
    "# Set the following quantization configurations:\n",
    "# Choose the desired QuantizationErrorMethod for the quantization parameters search.\n",
    "# Enable weights bias correction induced by quantization.\n",
    "# Enable shift negative corrections for improving 'signed' non-linear functions quantization (such as swish, prelu, etc.) \n",
    "# Set the threshold to filter outliers with z_score of 16. \n",
    "q_config = mct.core.QuantizationConfig(activation_error_method=QuantizationErrorMethod.MSE,\n",
    "                                       weights_error_method=QuantizationErrorMethod.MSE,\n",
    "                                       weights_bias_correction=True,\n",
    "                                       shift_negative_activation_correction=True,\n",
    "                                       z_threshold=16)\n",
    "\n",
    "ptq_config = mct.core.CoreConfig(quantization_config=q_config)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2edacb5b7779e4d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run model Post-Training Quantization\n",
    "Lastly, we quantize our model using MCT's post-training quantization API."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6162dd6dd1fce7ab"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f8373a-82a5-4b97-9a10-25ee2341d148",
   "metadata": {
    "id": "33f8373a-82a5-4b97-9a10-25ee2341d148"
   },
   "outputs": [],
   "source": [
    "quantized_model, quantization_info = mct.ptq.keras_post_training_quantization(\n",
    "    in_model=float_model, \n",
    "    representative_data_gen=representative_dataset_gen, \n",
    "    core_config=ptq_config, \n",
    "    target_platform_capabilities=tpc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7382ada6-d001-4564-907d-767fa4e9ec56",
   "metadata": {
    "id": "7382ada6-d001-4564-907d-767fa4e9ec56"
   },
   "source": [
    "That's it! Our model is now quantized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7a5150-3b92-49b5-abb2-06e6c5c91d6b",
   "metadata": {
    "id": "5a7a5150-3b92-49b5-abb2-06e6c5c91d6b"
   },
   "source": [
    "## Model Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce4fc61-e13c-48be-9f7c-d441ad76a386",
   "metadata": {
    "id": "0ce4fc61-e13c-48be-9f7c-d441ad76a386"
   },
   "source": [
    "In order to convert our model, we first need to export the it to Keras. Please ensure that the `save_model_path` has been set correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7c875-c4fc-4819-97e5-721805cba546",
   "metadata": {
    "tags": [],
    "id": "eef7c875-c4fc-4819-97e5-721805cba546"
   },
   "outputs": [],
   "source": [
    "quantized_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])"
    "save_folder=\"./mobilenet_tf\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "keras_path = os.path.join(save_folder, 'qmodel.keras')\n",
    "mct.exporter.keras_export_model(model=quantized_model, save_model_path=keras_path)"
   ]
  },
  {

   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "# Check if Java is installed\n",
    "result = subprocess.run([\"java\", \"-version\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "if result.returncode != 0:\n",
    "    from IPython.display import display, HTML\n",
    "    display(\n",
    "        HTML(\"<p style='color: red; font-weight: bold;'>Java is not installed. Please install Java 17 to proceed.</p>\"))\n",
    "    raise SystemExit(\"Stopping execution: Java is not installed.\")"
   ],
   "id": "511c77e540ee0eb8",
   "outputs": []
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java is installed:\n",
      "openjdk version \"17.0.13\" 2024-10-15\n",
      "OpenJDK Runtime Environment Homebrew (build 17.0.13+0)\n",
      "OpenJDK 64-Bit Server VM Homebrew (build 17.0.13+0, mixed mode, sharing)\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T09:29:16.778080Z",
     "start_time": "2025-01-15T09:28:53.918996Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "import sys\n",
    "cmd = [\"imxconv-tf\", \"-i\", keras_path,  \"-o\", save_folder, \"--overwrite-output\"]\n",
    "\n",
    "env_bin_path = os.path.dirname(sys.executable)\n",
    "os.environ[\"PATH\"] = f\"{env_bin_path}:{os.environ['PATH']}\"\n",
    "env = os.environ.copy()\n",
    "\n",
    "subprocess.run(cmd, env=env, check=True)"
   ],
   "id": "eb377735410760eb",
   "outputs": []
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['imxconv-tf', '-i', './mobilenet_tf/qmodel.keras', '-o', './mobilenet_tf', '--overwrite-output'], returncode=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "14877777",
   "metadata": {
    "id": "14877777"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7e1572",
   "metadata": {
    "id": "bb7e1572"
   },
   "source": [
    "In this tutorial, we demonstrated how to quantize a pre-trained model using MCT with a few lines of code. We saw that we can achieve an x4 compression ratio with minimal performance degradation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c1645e-205c-4d9a-8af3-e497b3addec1",
   "metadata": {
    "id": "01c1645e-205c-4d9a-8af3-e497b3addec1"
   },
   "source": [
    "\n",
    "\n",
    "Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
