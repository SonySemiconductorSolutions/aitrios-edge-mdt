{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8194007-6ea7-4e00-8931-a37ca2d0dd20",
   "metadata": {
    "id": "f8194007-6ea7-4e00-8931-a37ca2d0dd20"
   },
   "source": [
    "# Post-Training Quantization Example of MobileNetV2 Keras Model"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "7cc5c60a5a9d787f"
  },
  {
   "cell_type": "markdown",
   "id": "930e6d6d-4980-4d66-beed-9ff5a494acf9",
   "metadata": {
    "id": "930e6d6d-4980-4d66-beed-9ff5a494acf9"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699be4fd-d382-4eec-9d3f-e2e85cfb1762",
   "metadata": {
    "id": "699be4fd-d382-4eec-9d3f-e2e85cfb1762"
   },
   "source": [
    "This tutorial demonstrates a pre-trained model quantization using the **Model Compression Toolkit (MCT)**. \n",
    "\n",
    "It is done using the MCT's **Post-Training Quantization** tool. \n",
    "\n",
    "As we will see, post-training quantization is a low complexity yet effective quantization scheme. \n",
    "\n",
    "In this example, we quantize the model and evaluate the accuracy before and after quantization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85199e25-c587-41b1-aaf5-e1d23ce97ca1",
   "metadata": {
    "id": "85199e25-c587-41b1-aaf5-e1d23ce97ca1"
   },
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0e9543-d356-412f-acf1-c2ecad553e06",
   "metadata": {
    "id": "9c0e9543-d356-412f-acf1-c2ecad553e06"
   },
   "source": [
    "In this tutorial we cover the following subjects:\n",
    "\n",
    "1. Post-Training Quantization using MCT.\n",
    "2. Loading and preprocessing ImageNet's validation dataset.\n",
    "3. Constructing an unlabeled representative dataset.\n",
    "4. Accuracy evaluation of the floating-point and the quantized models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04228b7c-00f1-4ded-bead-722e2a4e89a0",
   "metadata": {
    "tags": [],
    "id": "04228b7c-00f1-4ded-bead-722e2a4e89a0"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2657cf1a-654d-45a6-b877-8bf42fc26d0d",
   "metadata": {
    "id": "2657cf1a-654d-45a6-b877-8bf42fc26d0d"
   },
   "source": [
    "Install and import the relevant packages:\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "324685b9-5dcc-4d22-80f4-dec9a93d3324",
   "metadata": {
    "tags": [],
    "id": "324685b9-5dcc-4d22-80f4-dec9a93d3324",
    "ExecuteTime": {
     "end_time": "2025-01-14T14:38:40.726611Z",
     "start_time": "2025-01-14T14:38:40.486387Z"
    }
   },
   "source": [
    "import os\n",
    "from importlib import util\n",
    "\n",
    "\n",
    "TF_VER = os.getenv('TF_VER', '2.15.1')\n",
    "IMX500_AI_TOOLCHAIN_VER = os.getenv('IMX500_AI_TOOLCHAIN_VERSION', '0.0.0.dev0')\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except ImportError:\n",
    "    print(f\"Installing TensorFlow {TF_VER}\")\n",
    "    !pip install tensorflow=={TF_VER}\n",
    "\n",
    "if not util.find_spec('imx500_ai_toolchain') or not util.find_spec(\"uni.tensorflow\"):\n",
    "    print(f\"Installing imx500_ai_toolchain {IMX500_AI_TOOLCHAIN_VER}\")\n",
    "    !pip install imx500_ai_toolchain[tf]\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing TensorFlow 2.15.1\n",
      "zsh:1: command not found: pip\r\n",
      "Installing imx500_ai_toolchain 0.0.0.dev0\n",
      "zsh:1: no matches found: imx500_ai_toolchain[tf]\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "b3f0acc8-281c-4bca-b0b9-3d7677105f19",
   "metadata": {
    "id": "b3f0acc8-281c-4bca-b0b9-3d7677105f19",
    "ExecuteTime": {
     "end_time": "2025-01-14T14:38:40.864856Z",
     "start_time": "2025-01-14T14:38:40.729308Z"
    }
   },
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import model_compression_toolkit as mct\n",
    "import os"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mkeras\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmodel_compression_toolkit\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mmct\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "0c7fed0d-cfc8-41ee-adf1-22a98110397b",
   "metadata": {
    "id": "0c7fed0d-cfc8-41ee-adf1-22a98110397b"
   },
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download ImageNet dataset with only the validation split.\n",
    "\n",
    "**Note** that for demonstration purposes we use the validation set for the model quantization routines. Usually, a subset of the training dataset is used, but loading it is a heavy procedure that is unnecessary for the sake of this demonstration.\n",
    "\n",
    "This step may take several minutes..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "aecde59e4c37b1da"
  },
  {
   "cell_type": "code",
   "source": [
    "IMAGENET_DIR = os.getenv('IMAGENET_DIR', './imagenet')\n",
    "print(f\"using imagenet from: {IMAGENET_DIR}\")\n",
    "if not os.path.isdir(IMAGENET_DIR):\n",
    "    !mkdir imagenet\n",
    "    !wget https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz\n",
    "    !mv ILSVRC2012_devkit_t12.tar.gz imagenet/\n",
    "    !wget https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar\n",
    "    !mv ILSVRC2012_img_val.tar imagenet/"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T14:04:29.534187Z",
     "start_time": "2025-01-14T14:04:29.529550Z"
    }
   },
   "id": "5c18b26e293b085e",
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract ImageNet validation dataset using using 'prepare_imagenet.sh' script"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e48ff22f70ce997e"
  },
  {
   "cell_type": "code",
   "source": [
    "if not os.path.isdir('imagenet/val'):\n",
    "    import subprocess\n",
    "    !git clone https://github.com/sony/model_optimization.git temp_mct && mv temp_mct/tutorials . && \\rm -rf temp_mct\n",
    "    !chmod +x tutorials/resources/scripts/prepare_imagenet.sh\n",
    "    subprocess.run(['tutorials/resources/scripts/prepare_imagenet.sh'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T14:04:30.560703Z",
     "start_time": "2025-01-14T14:04:30.558152Z"
    }
   },
   "id": "5a6fb997c54aa3fb",
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "id": "028112db-3143-4fcb-96ae-e639e6476c31",
   "metadata": {
    "id": "028112db-3143-4fcb-96ae-e639e6476c31"
   },
   "source": [
    "Define the required preprocessing method for the pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "id": "ed56f505-97ff-4acb-8ad8-ef09c53e9d57",
   "metadata": {
    "id": "ed56f505-97ff-4acb-8ad8-ef09c53e9d57",
    "ExecuteTime": {
     "end_time": "2025-01-14T14:04:31.428375Z",
     "start_time": "2025-01-14T14:04:31.426720Z"
    }
   },
   "source": [
    "def imagenet_preprocess_input(images, labels):\n",
    "    return tf.keras.applications.mobilenet_v2.preprocess_input(images), labels"
   ],
   "outputs": [],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Representative dataset construction\n",
    "We show how to create a generator for the representative dataset, which is required for post-training quantization.\n",
    "\n",
    "The representative dataset is used for collecting statistics on the inference outputs of all layers in the model.\n",
    " \n",
    "In order to decide on the size of the representative dataset, we configure the batch size and the number of calibration iterations.\n",
    "This gives us the total number of samples that will be used during PTQ (batch_size x n_iter).\n",
    "In this example we set `batch_size = 50` and `n_iter = 10`, resulting in a total of 500 representative images.\n",
    "\n",
    "Please ensure that the dataset path has been set correctly."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fcbb3eecae5346a9"
  },
  {
   "cell_type": "code",
   "id": "0408f624-ab68-4989-95f8-f9d327882840",
   "metadata": {
    "id": "0408f624-ab68-4989-95f8-f9d327882840",
    "ExecuteTime": {
     "end_time": "2025-01-14T14:04:34.520048Z",
     "start_time": "2025-01-14T14:04:32.373612Z"
    }
   },
   "source": [
    "from typing import Generator\n",
    "\n",
    "REPRESENTATIVE_DATASET_FOLDER = os.path.join(IMAGENET_DIR, 'val')\n",
    "BATCH_SIZE = 50\n",
    "n_iter=10\n",
    "\n",
    "# Create representative dataset generator\n",
    "def get_representative_dataset() -> Generator:\n",
    "    \"\"\"A function that loads the dataset and returns a representative dataset generator.\n",
    "\n",
    "    Returns:\n",
    "        Generator: A generator yielding batches of preprocessed images.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the dataset from folder\n",
    "    print('loading dataset, this may take few minutes ...')    \n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory=REPRESENTATIVE_DATASET_FOLDER,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        image_size=[224, 224],\n",
    "        shuffle=True,\n",
    "        crop_to_aspect_ratio=True,\n",
    "        interpolation='bilinear')  \n",
    "    # Preprocess the data\n",
    "    dataset = dataset.map(lambda x, y: (imagenet_preprocess_input(x, y)))\n",
    "\n",
    "    def representative_dataset() -> Generator:\n",
    "        \"\"\"A generator function that yields batch of preprocessed images.\n",
    "\n",
    "        Yields:\n",
    "            A batch of preprocessed images.\n",
    "        \"\"\"\n",
    "        for _ in range(n_iter):\n",
    "            yield dataset.take(1).get_single_element()[0].numpy()\n",
    "\n",
    "    return representative_dataset\n",
    "\n",
    "# Create a representative dataset generator\n",
    "representative_dataset_gen = get_representative_dataset()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading dataset, this may take few minutes ...\n",
      "Found 100000 files belonging to 1001 classes.\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "4a1e9ba6-2954-4506-ad5c-0da273701ba5",
   "metadata": {
    "id": "4a1e9ba6-2954-4506-ad5c-0da273701ba5"
   },
   "source": [
    "## Model Post-Training quantization using MCT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55edbb99-ab2f-4dde-aa74-4ddee61b2615",
   "metadata": {
    "id": "55edbb99-ab2f-4dde-aa74-4ddee61b2615"
   },
   "source": [
    "This is the main part in which we quantize our model.\n",
    "\n",
    "First, we load a pre-trained MobileNetV2 model from Keras, in 32-bits floating-point precision format."
   ]
  },
  {
   "cell_type": "code",
   "id": "80cac59f-ec5e-41ca-b673-96220924a47c",
   "metadata": {
    "id": "80cac59f-ec5e-41ca-b673-96220924a47c",
    "ExecuteTime": {
     "end_time": "2025-01-14T14:04:34.745074Z",
     "start_time": "2025-01-14T14:04:34.522544Z"
    }
   },
   "source": [
    "from keras.applications.mobilenet_v2 import MobileNetV2\n",
    "float_model = MobileNetV2()"
   ],
   "outputs": [],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "8a8b486a-ca39-45d9-8699-f7116b0414c9",
   "metadata": {
    "id": "8a8b486a-ca39-45d9-8699-f7116b0414c9"
   },
   "source": [
    "Next, we need to define a `TargetPlatformCapability` object, representing the HW specifications on which we wish to eventually deploy our quantized model.\n",
    "\n",
    "In addition, we need to define the Quantization Configuration for our PTQ routine.\n",
    "\n",
    "Here, we demonstrate how to define a quantization configuration with several key argument that can be controlled by the user.\n",
    "**Note** that you can skip this part if you prefer to use the default quantization settings."
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from model_compression_toolkit.core import QuantizationErrorMethod\n",
    "\n",
    "# Specify the IMX500-v1 target platform capability (TPC) \n",
    "tpc = mct.get_target_platform_capabilities(\"tensorflow\", 'imx500', target_platform_version='v1')\n",
    "\n",
    "# Set the following quantization configurations:\n",
    "# Choose the desired QuantizationErrorMethod for the quantization parameters search.\n",
    "# Enable weights bias correction induced by quantization.\n",
    "# Enable shift negative corrections for improving 'signed' non-linear functions quantization (such as swish, prelu, etc.) \n",
    "# Set the threshold to filter outliers with z_score of 16. \n",
    "q_config = mct.core.QuantizationConfig(activation_error_method=QuantizationErrorMethod.MSE,\n",
    "                                       weights_error_method=QuantizationErrorMethod.MSE,\n",
    "                                       weights_bias_correction=True,\n",
    "                                       shift_negative_activation_correction=True,\n",
    "                                       z_threshold=16)\n",
    "\n",
    "ptq_config = mct.core.CoreConfig(quantization_config=q_config)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T14:04:34.750873Z",
     "start_time": "2025-01-14T14:04:34.749088Z"
    }
   },
   "id": "2edacb5b7779e4d8",
   "outputs": [],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run model Post-Training Quantization\n",
    "Lastly, we quantize our model using MCT's post-training quantization API."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6162dd6dd1fce7ab"
  },
  {
   "cell_type": "code",
   "id": "33f8373a-82a5-4b97-9a10-25ee2341d148",
   "metadata": {
    "id": "33f8373a-82a5-4b97-9a10-25ee2341d148",
    "ExecuteTime": {
     "end_time": "2025-01-14T14:05:20.649899Z",
     "start_time": "2025-01-14T14:04:36.342486Z"
    }
   },
   "source": [
    "quantized_model, quantization_info = mct.ptq.keras_post_training_quantization(\n",
    "    in_model=float_model, \n",
    "    representative_data_gen=representative_dataset_gen, \n",
    "    core_config=ptq_config, \n",
    "    target_platform_capabilities=tpc)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Statistics Collection: 10it [00:35,  3.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running quantization parameters search. This process might take some time, depending on the model size and the selected quantization methods.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating quantization parameters: 100%|██████████| 105/105 [00:07<00:00, 13.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please run your accuracy evaluation on the exported quantized model to verify it's accuracy.\n",
      "Checkout the FAQ and Troubleshooting pages for resolving common issues and improving the quantized model accuracy:\n",
      "FAQ: https://github.com/sony/model_optimization/tree/main/FAQ.md\n",
      "Quantization Troubleshooting: https://github.com/sony/model_optimization/tree/main/quantization_troubleshooting.md\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "id": "7382ada6-d001-4564-907d-767fa4e9ec56",
   "metadata": {
    "id": "7382ada6-d001-4564-907d-767fa4e9ec56"
   },
   "source": [
    "That's it! Our model is now quantized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7a5150-3b92-49b5-abb2-06e6c5c91d6b",
   "metadata": {
    "id": "5a7a5150-3b92-49b5-abb2-06e6c5c91d6b"
   },
   "source": [
    "## Models evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce4fc61-e13c-48be-9f7c-d441ad76a386",
   "metadata": {
    "id": "0ce4fc61-e13c-48be-9f7c-d441ad76a386"
   },
   "source": [
    "In order to evaluate our models, we first need to load the validation dataset. As before, please ensure that the dataset path has been set correctly."
   ]
  },
  {
   "cell_type": "code",
   "id": "eef7c875-c4fc-4819-97e5-721805cba546",
   "metadata": {
    "tags": [],
    "id": "eef7c875-c4fc-4819-97e5-721805cba546",
    "ExecuteTime": {
     "end_time": "2025-01-14T14:05:22.809921Z",
     "start_time": "2025-01-14T14:05:20.656252Z"
    }
   },
   "source": [
    "TEST_DATASET_FOLDER = os.path.join(IMAGENET_DIR, 'val')\n",
    "def get_validation_dataset() -> tf.data.Dataset:\n",
    "    \"\"\"Load the validation dataset for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: The validation dataset.\n",
    "    \"\"\"\n",
    "    dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory=TEST_DATASET_FOLDER,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        image_size=[224, 224],\n",
    "        shuffle=False,\n",
    "        crop_to_aspect_ratio=True,\n",
    "        interpolation='bilinear')\n",
    "    dataset = dataset.map(lambda x, y: (imagenet_preprocess_input(x, y)))\n",
    "    return dataset\n",
    "\n",
    "evaluation_dataset = get_validation_dataset()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 100000 files belonging to 1001 classes.\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can export the model to Keras and TFLite. Please ensure that the `save_model_path` has been set correctly."
   ],
   "metadata": {
    "id": "6YjIdiRRjgkL"
   },
   "id": "6YjIdiRRjgkL"
  },
  {
   "metadata": {
    "id": "z3CA16-ojoFL",
    "ExecuteTime": {
     "end_time": "2025-01-14T14:05:23.018830Z",
     "start_time": "2025-01-14T14:05:22.814116Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting Keras model with MCTQ custom quantizers to: ./qmodel.keras\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31,
   "source": [
    "float_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])\n",
    "quantized_model.compile(loss=keras.losses.SparseCategoricalCrossentropy(), metrics=[\"accuracy\"])"
   ],
   "id": "z3CA16-ojoFL"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T14:51:23.630514Z",
     "start_time": "2025-01-14T14:51:23.618071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "save_folder=\"./mobilenet_tf\"\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "keras_path = os.path.join(save_folder, 'qmodel.keras')\n",
    "mct.exporter.keras_export_model(model=quantized_model, save_model_path=keras_path)"
   ],
   "id": "3404550d42385aa3",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mct' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 4\u001B[0m\n\u001B[1;32m      2\u001B[0m os\u001B[38;5;241m.\u001B[39mmakedirs(save_folder, exist_ok\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m      3\u001B[0m keras_path \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(save_folder, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mqmodel.keras\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 4\u001B[0m mct\u001B[38;5;241m.\u001B[39mexporter\u001B[38;5;241m.\u001B[39mkeras_export_model(model\u001B[38;5;241m=\u001B[39mquantized_model, save_model_path\u001B[38;5;241m=\u001B[39mkeras_path)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'mct' is not defined"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import subprocess\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "try:\n",
    "    # Check if Java is installed\n",
    "    result = subprocess.run([\"java\", \"-version\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"Java is installed:\")\n",
    "        print(result.stderr.strip())  # Java version details are typically in stderr\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "except FileNotFoundError:\n",
    "    # Display an error message and halt further execution\n",
    "    display(\n",
    "        HTML(\"<p style='color: red; font-weight: bold;'>Java is not installed. Please install Java 17 to proceed.</p>\"))\n",
    "    raise SystemExit(\"Stopping execution: Java is not installed.\")"
   ],
   "id": "511c77e540ee0eb8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import subprocess\n",
    "import sys\n",
    "cmd = [\"imxconv-tf\", \"-i\", keras_path,  \"-o\", save_folder, \"--overwrite-output\"]\n",
    "\n",
    "env_bin_path = os.path.dirname(sys.executable)\n",
    "os.environ[\"PATH\"] = f\"{env_bin_path}:{os.environ['PATH']}\"\n",
    "env = os.environ.copy()\n",
    "\n",
    "subprocess.run(cmd, env=env, check=True)"
   ],
   "id": "eb377735410760eb"
  },
  {
   "cell_type": "markdown",
   "id": "14877777",
   "metadata": {
    "id": "14877777"
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7e1572",
   "metadata": {
    "id": "bb7e1572"
   },
   "source": [
    "In this tutorial, we demonstrated how to quantize a pre-trained model using MCT with a few lines of code. We saw that we can achieve an x4 compression ratio with minimal performance degradation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c1645e-205c-4d9a-8af3-e497b3addec1",
   "metadata": {
    "id": "01c1645e-205c-4d9a-8af3-e497b3addec1"
   },
   "source": [
    "\n",
    "\n",
    "Copyright 2022 Sony Semiconductor Israel, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2b45178ca8e06931"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
