{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Post-Training Quantization in PyTorch using the Model Compression Toolkit (MCT)\n",
    "\n",
    "\n",
    "## Overview\n",
    "This quick-start guide explains how to use the **Model Compression Toolkit (MCT)** to quantize a PyTorch model. We will load a pre-trained model and  quantize it using the MCT with **Post-Training Quatntization (PTQ)**. Finally, we will evaluate the quantized model and export it to an ONNX file.\n",
    "\n",
    "## Summary\n",
    "In this tutorial, we will cover:\n",
    "\n",
    "1. Loading and preprocessing ImageNet’s validation dataset.\n",
    "2. Constructing an unlabeled representative dataset.\n",
    "3. Post-Training Quantization using MCT.\n",
    "4. Accuracy evaluation of the floating-point and the quantized models.\n",
    "\n",
    "## Setup\n",
    "Install the relevant packages:"
   ],
   "id": "7cf96fb4"
  },
  {
   "cell_type": "code",
   "id": "5441efd2978cea5a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T17:41:13.488717Z",
     "start_time": "2025-01-14T17:41:13.484692Z"
    }
   },
   "source": [
    "import os\n",
    "from importlib import util\n",
    "\n",
    "IMX500_AI_TOOLCHAIN_VER = os.getenv('IMX500_AI_TOOLCHAIN_VERSION', '0.0.0.dev0')\n",
    "\n",
    "if not util.find_spec('imx500_ai_toolchain') or not util.find_spec(\"uni.pytorch\"):\n",
    "    print(f\"Installing imx500_ai_toolchain {IMX500_AI_TOOLCHAIN_VER}\")\n",
    "    !pip install imx500_ai_toolchain[pt]\n",
    "\n",
    "if not util.find_spec('torch') or not util.find_spec(\"torchvision\"):\n",
    "    !pip install -q torch torchvision"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "a82928d0",
   "metadata": {
    "id": "a82928d0",
    "ExecuteTime": {
     "end_time": "2025-01-14T17:41:14.497126Z",
     "start_time": "2025-01-14T17:41:13.492327Z"
    }
   },
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torchvision.datasets import ImageNet\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load a pre-trained MobileNetV2 model from torchvision, in 32-bits floating-point precision format."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c2556ce8144e1d3"
  },
  {
   "cell_type": "code",
   "source": [
    "weights = MobileNet_V2_Weights.IMAGENET1K_V2\n",
    "\n",
    "float_model = mobilenet_v2(weights=weights)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T17:41:14.564250Z",
     "start_time": "2025-01-14T17:41:14.520593Z"
    }
   },
   "id": "7a302610146f1ec3",
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataset preparation\n",
    "### Download ImageNet validation set\n",
    "Download ImageNet dataset with only the validation split.\n",
    "\n",
    "**Note** that for demonstration purposes we use the validation set for the model quantization routines. Usually, a subset of the training dataset is used, but loading it is a heavy procedure that is unnecessary for the sake of this demonstration.\n",
    "\n",
    "This step may take several minutes..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4df074784266e12e"
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "IMAGENET_DIR = os.getenv('IMAGENET_DIR', './imagenet')\n",
    "print(f\"using imagenet from: {IMAGENET_DIR}\")\n",
    "\n",
    "if not os.path.isdir(IMAGENET_DIR):\n",
    "    !mkdir imagenet\n",
    "    !wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_devkit_t12.tar.gz\n",
    "    !wget -P imagenet https://image-net.org/data/ILSVRC/2012/ILSVRC2012_img_val.tar"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T17:41:14.571305Z",
     "start_time": "2025-01-14T17:41:14.569038Z"
    }
   },
   "id": "a8a3327f28c20caf",
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "Extract ImageNet validation dataset using torchvision \"datasets\" module."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ff2ea33659f0c1a"
  },
  {
   "cell_type": "code",
   "source": "dataset = ImageNet(root=IMAGENET_DIR, split='val', transform=weights.transforms())",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T17:41:14.845936Z",
     "start_time": "2025-01-14T17:41:14.754604Z"
    }
   },
   "id": "18f57edc3b87cad3",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "c0321aad",
   "metadata": {
    "id": "c0321aad"
   },
   "source": [
    "## Representative Dataset\n",
    "For quantization with MCT, we need to define a representative dataset required by the PTQ algorithm. This dataset is a generator that returns a list of images:"
   ]
  },
  {
   "cell_type": "code",
   "id": "618975be",
   "metadata": {
    "id": "618975be",
    "ExecuteTime": {
     "end_time": "2025-01-14T17:41:15.724658Z",
     "start_time": "2025-01-14T17:41:15.722634Z"
    }
   },
   "source": [
    "batch_size = 16\n",
    "n_iter = 10\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "def representative_dataset_gen():\n",
    "    dataloader_iter = iter(dataloader)\n",
    "    for _ in range(n_iter):\n",
    "        yield [next(dataloader_iter)[0]]\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Target Platform Capabilities (TPC)\n",
    "In addition, MCT optimizes the model for dedicated hardware platforms. This is done using TPC (for more details, please visit our [documentation](https://sony.github.io/model_optimization/api/api_docs/modules/target_platform.html)). Here, we use the default Pytorch TPC:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33271e23c3eff3b5"
  },
  {
   "cell_type": "code",
   "source": [
    "import model_compression_toolkit as mct\n",
    "\n",
    "# Get a FrameworkQuantizationCapabilities object that models the hardware platform for the quantized model inference. Here, for example, we use the default platform that is attached to a Pytorch layers representation.\n",
    "target_platform_cap = mct.get_target_platform_capabilities('pytorch', 'default')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-01-14T17:41:17.568693Z",
     "start_time": "2025-01-14T17:41:16.622891Z"
    }
   },
   "id": "ae04779a863facd7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "d0a92bee",
   "metadata": {
    "id": "d0a92bee"
   },
   "source": [
    "## Post-Training Quantization using MCT\n",
    "Now for the exciting part! Let’s run PTQ on the model. "
   ]
  },
  {
   "cell_type": "code",
   "id": "63f695dd",
   "metadata": {
    "id": "63f695dd",
    "ExecuteTime": {
     "end_time": "2025-01-14T17:41:50.124035Z",
     "start_time": "2025-01-14T17:41:17.573104Z"
    }
   },
   "source": [
    "quantized_model, quantization_info = mct.ptq.pytorch_post_training_quantization(\n",
    "        in_module=float_model,\n",
    "        representative_data_gen=representative_dataset_gen,\n",
    "        target_platform_capabilities=target_platform_cap\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Statistics Collection: 10it [00:21,  2.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running quantization parameters search. This process might take some time, depending on the model size and the selected quantization methods.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating quantization parameters: 100%|██████████| 102/102 [00:09<00:00, 11.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Please run your accuracy evaluation on the exported quantized model to verify it's accuracy.\n",
      "Checkout the FAQ and Troubleshooting pages for resolving common issues and improving the quantized model accuracy:\n",
      "FAQ: https://github.com/sony/model_optimization/tree/main/FAQ.md\n",
      "Quantization Troubleshooting: https://github.com/sony/model_optimization/tree/main/quantization_troubleshooting.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "d3521637",
   "metadata": {
    "id": "d3521637"
   },
   "source": [
    "Our model is now quantized. MCT has created a simulated quantized model within the original PyTorch framework by inserting [quantization representation modules](https://github.com/sony/mct_quantizers). These modules, such as `PytorchQuantizationWrapper` and `PytorchActivationQuantizationHolder`, wrap PyTorch layers to simulate the quantization of weights and activations, respectively. While the size of the saved model remains unchanged, all the quantization parameters are stored within these modules and are ready for deployment on the target hardware. In this example, we used the default MCT settings, which compressed the model from 32 bits to 8 bits, resulting in a compression ratio of 4x. Let's print the quantized model and examine the quantization modules:"
   ]
  },
  {
   "cell_type": "code",
   "id": "oXMn6bFjbQad",
   "metadata": {
    "id": "oXMn6bFjbQad",
    "ExecuteTime": {
     "end_time": "2025-01-14T17:41:51.997110Z",
     "start_time": "2025-01-14T17:41:50.132482Z"
    }
   },
   "source": [
    "save_folder = './mobilenet_pt'\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "onnx_path = os.path.join(save_folder, 'qmodel.onnx')\n",
    "mct.exporter.pytorch_export_model(quantized_model, save_model_path=onnx_path, repr_dataset=representative_dataset_gen)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting onnx model with MCTQ quantizers: ./mobilenet_pt/qmodel.onnx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chizkiyahu/envs/wr_pt/lib/python3.11/site-packages/mct_quantizers/pytorch/quantizers/weights_inferable_quantizers/weights_symmetric_inferable_quantizer.py:52: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  threshold = torch.tensor(threshold, dtype=torch.float32).to(get_working_device())\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T17:41:52.075226Z",
     "start_time": "2025-01-14T17:41:52.002299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "try:\n",
    "    # Check if Java is installed\n",
    "    result = subprocess.run([\"java\", \"-version\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"Java is installed:\")\n",
    "        print(result.stderr.strip())  # Java version details are typically in stderr\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "except FileNotFoundError:\n",
    "    # Display an error message and halt further execution\n",
    "    display(\n",
    "        HTML(\"<p style='color: red; font-weight: bold;'>Java is not installed. Please install Java 17 to proceed.</p>\"))\n",
    "    raise SystemExit(\"Stopping execution: Java is not installed.\")"
   ],
   "id": "fd03b5f79105d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Java is installed:\n",
      "openjdk version \"17.0.13\" 2024-10-15\n",
      "OpenJDK Runtime Environment Homebrew (build 17.0.13+0)\n",
      "OpenJDK 64-Bit Server VM Homebrew (build 17.0.13+0, mixed mode, sharing)\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T17:42:15.401035Z",
     "start_time": "2025-01-14T17:41:52.082575Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "import sys\n",
    "cmd = [\"imxconv-pt\", \"-i\", onnx_path,  \"-o\", save_folder, \"--overwrite-output\"]\n",
    "\n",
    "env_bin_path = os.path.dirname(sys.executable)\n",
    "os.environ[\"PATH\"] = f\"{env_bin_path}:{os.environ['PATH']}\"\n",
    "env = os.environ.copy()\n",
    "\n",
    "subprocess.run(cmd, env=env, check=True)"
   ],
   "id": "26051e8af246d7af",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-14 19:41:52,786 INFO : Running version 1.10.0 [/Users/chizkiyahu/envs/wr_pt/lib/python3.11/site-packages/uni/common/logger.py:148]\n",
      "2025-01-14 19:41:52,786 INFO : Converting mobilenet_pt/qmodel.onnx [/Users/chizkiyahu/envs/wr_pt/lib/python3.11/site-packages/uni/common/logger.py:148]\n",
      "2025-01-14 19:41:54,838 INFO : Wrote outputs to /var/folders/pp/xsvkwn4n6dz94h8y0_nrw_fw0000gn/T/tmpvwiazter/qmodel.uni-pytorch.um.pb [/Users/chizkiyahu/envs/wr_pt/lib/python3.11/site-packages/uni/common/logger.py:148]\n",
      "2025-01-14 19:41:54,851 INFO : Converted successfully [/Users/chizkiyahu/envs/wr_pt/lib/python3.11/site-packages/uni/common/logger.py:148]\n",
      "2025-01-14 19:41:55,232 INFO : CODE: [START] Starting SDSPconv\n",
      "2025-01-14 19:42:03,696 INFO : ConvFe conversion finished successfully\n",
      "2025-01-14 19:42:04,084 INFO : CBE component - DspConvParser has started conversion.\n",
      "2025-01-14 19:42:04,217 INFO : Dsp-Dnn-Parser finished successfully !\n",
      "2025-01-14 19:42:05,047 INFO : LogicModel generated successfully ! \n",
      "2025-01-14 19:42:05,047 INFO : LogicModel path is: /Users/chizkiyahu/work_code/IMX500-AI-Toolchain-wrapper/tutorials/./mobilenet_pt/tempConverterFm2Lm/qmodel\n",
      "2025-01-14 19:42:05,048 INFO : DspConvParser runs: 974 [msec]\n",
      "2025-01-14 19:42:05,048 INFO : CBE component - DspConvParser finished conversion.\n",
      "2025-01-14 19:42:14,817 INFO : Converter Backend successfully completed!\n",
      "2025-01-14 19:42:14,818 INFO : Conversion time is 9.69 Seconds\n",
      "2025-01-14 19:42:15,216 INFO : packer zip successfully generated under /Users/chizkiyahu/work_code/IMX500-AI-Toolchain-wrapper/tutorials/mobilenet_pt/packerOut.zip\n",
      "2025-01-14 19:42:15,239 INFO : CODE: [OUTPUT] Output is in /Users/chizkiyahu/work_code/IMX500-AI-Toolchain-wrapper/tutorials/./mobilenet_pt\n",
      "2025-01-14 19:42:15,248 INFO : CODE: [DONE] Done (20s)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args=['imxconv-pt', '-i', './mobilenet_pt/qmodel.onnx', '-o', './mobilenet_pt', '--overwrite-output'], returncode=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "id": "bb7e1572",
   "metadata": {
    "id": "bb7e1572"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we demonstrated how to quantize a classification model for MNIST in a hardware-friendly manner using MCT. We observed that a 4x compression ratio was achieved with minimal performance degradation.\n",
    "\n",
    "The key advantage of hardware-friendly quantization is that the model can run more efficiently in terms of runtime, power consumption, and memory usage on designated hardware.\n",
    "\n",
    "MCT can deliver competitive results across a wide range of tasks and network architectures. For more details, [check out the paper:](https://arxiv.org/abs/2109.09113).\n",
    "\n",
    "## Copyrights\n",
    "\n",
    "Copyright 2024 Sony Semiconductor Israel, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "e0d17b025c5ba5a9"
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
