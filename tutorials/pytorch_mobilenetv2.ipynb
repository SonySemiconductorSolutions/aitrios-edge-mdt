{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Post-Training Quantization + Conversion to IMX500 of a MobileNetV2 PyTorch Model\n",
    "\n",
    "\n",
    "## Overview\n",
    "This tutorial demonstrates how to apply Post Training Quantization to a PyTorch pretrained model using the [**Model Compression Toolkit (MCT)**](https://github.com/sony/model_optimization) and how to convert the resulting model to a binary format suitable to load to IMX500 using the [**IMX500-converter**](https://developer.aitrios.sony-semicon.com/en/raspberrypi-ai-camera/documentation/imx500-converter?version=3.14.3&progLang=) . \n",
    "\n",
    "This example is not intended to demonstrate evaluating MCT PTQ performance and as such intentionally uses generated random data  to speed up the process.\n",
    " \n",
    "For the full tutorial on MCT's PTQ  see - [*MCT PTQ PyTorch Tutorial*](https://github.com/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/pytorch/example_pytorch_post_training_quantization.ipynb)\n",
    "\n",
    "For tutorials on other quantization features of MCT see [*MCT Features Tutorials*](https://github.com/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/README.md)\n",
    "\n",
    "## Summary\n",
    "In this tutorial we cover the following steps:\n",
    "\n",
    "1. Post-Training Quantization using MCT.\n",
    "2. Converting the model to a IMX500 suitable representation using IMX500-Converter\n",
    "\n",
    "## Setup\n",
    "Install the relevant packages:"
   ],
   "id": "7cf96fb4"
  },
  {
   "cell_type": "code",
   "id": "5441efd2978cea5a",
   "metadata": {},
   "source": [
    "from importlib import util\n",
    "\n",
    "if not util.find_spec('edge_mdt') or not util.find_spec(\"uni.pytorch\"):\n",
    "    print(f\"Installing edge-mdt\")\n",
    "    !pip install edge-mdt[pt]\n",
    "\n",
    "if not util.find_spec('torch') or not util.find_spec(\"torchvision\"):\n",
    "    !pip install -q torch torchvision"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "!pip install -q onnx\n",
    "import torch\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights"
   ],
   "id": "a82928d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load a pre-trained MobileNetV2 model from torchvision, in 32-bits floating-point precision format."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c2556ce8144e1d3"
  },
  {
   "cell_type": "code",
   "source": [
    "weights = MobileNet_V2_Weights.IMAGENET1K_V2\n",
    "\n",
    "float_model = mobilenet_v2(weights=weights)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a302610146f1ec3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "## Dataset preparation\n",
   "metadata": {
    "collapsed": false
   },
   "id": "4df074784266e12e"
  },
  {
   "cell_type": "markdown",
   "id": "c0321aad",
   "metadata": {
    "id": "c0321aad"
   },
   "source": [
    "## Representative Dataset\n",
    "We're all set to use MCT's post-training quantization. To begin, we'll define a representative dataset generator. Please note that for demonstration purposes, we will generate random data of the desired image shape instead of using real images. Then, we will apply PTQ on our model using the dataset generator we have created. For more details on using MCT, refer to the MCT tutorials"
   ]
  },
  {
   "cell_type": "code",
   "id": "618975be",
   "metadata": {
    "id": "618975be"
   },
   "source": [
    "from typing import Iterator, List\n",
    " \n",
    "NUM_ITERS = 20\n",
    "BATCH_SIZE = 32\n",
    "def get_representative_dataset(n_iter: int):\n",
    "    \"\"\"\n",
    "    This function creates a representative dataset generator. The generator yields numpy\n",
    "        arrays of batches of shape: [Batch, C, H, W].\n",
    "    Args:\n",
    "        n_iter: number of iterations for MCT to calibrate on\n",
    "    Returns:\n",
    "        A representative dataset generator\n",
    "    \"\"\"\n",
    "    def representative_dataset() -> Iterator[List]:\n",
    "        for _ in range(n_iter):\n",
    "            yield [torch.rand(BATCH_SIZE, 3, 224, 224)]\n",
    "    return representative_dataset\n",
    "representative_data_generator = get_representative_dataset(n_iter=NUM_ITERS)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Target Platform Capabilities (TPC)\n",
    "In addition, MCT optimizes the model for dedicated hardware platforms. This is done using TPC (for more details, please visit our [documentation](https://github.com/SonySemiconductorSolutions/aitrios-edge-mdt-tpc))."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33271e23c3eff3b5"
  },
  {
   "cell_type": "code",
   "source": [
    "from edgemdt_tpc import get_target_platform_capabilities\n",
    "import model_compression_toolkit as mct\n",
    "\n",
    "# Get a TPC object representing the imx500 hardware and use it for PyTorch model quantization in MCT\n",
    "tpc = get_target_platform_capabilities(tpc_version='1.0', device_type='imx500')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae04779a863facd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d0a92bee",
   "metadata": {
    "id": "d0a92bee"
   },
   "source": [
    "## Post-Training Quantization using MCT\n",
    "Now for the exciting part! Letâ€™s run PTQ on the model. "
   ]
  },
  {
   "cell_type": "code",
   "id": "63f695dd",
   "metadata": {
    "id": "63f695dd"
   },
   "source": [
    "quantized_model, quantization_info = mct.ptq.pytorch_post_training_quantization(\n",
    "        in_module=float_model,\n",
    "        representative_data_gen=representative_data_generator,\n",
    "        target_platform_capabilities=tpc\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d3521637",
   "metadata": {
    "id": "d3521637"
   },
   "source": "Our model is now quantized. MCT has created a simulated quantized model within the original PyTorch framework by inserting [quantization representation modules](https://github.com/sony/mct_quantizers). These modules, such as `PytorchQuantizationWrapper` and `PytorchActivationQuantizationHolder`, wrap PyTorch layers to simulate the quantization of weights and activations, respectively. While the size of the saved model remains unchanged, all the quantization parameters are stored within these modules and are ready for deployment on the target hardware. In this example, we used the default MCT settings, which compressed the model from 32 bits to 8 bits, resulting in a compression ratio of 4x. "
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Conversion",
   "id": "b7871d993d7bb82b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Exporting to ONNX serialization \n",
    "In order to convert our model to an binary suitable to load to IMX500, we first need to serialize it to ONNX format. Please ensure that the `save_model_path` has been set correctly."
   ],
   "id": "6c40b2f86859fbc1"
  },
  {
   "cell_type": "code",
   "id": "oXMn6bFjbQad",
   "metadata": {
    "id": "oXMn6bFjbQad"
   },
   "source": [
    "import os\n",
    "import model_compression_toolkit as mct\n",
    "save_folder = './mobilenet_pt'\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "onnx_path = os.path.join(save_folder, 'qmodel.onnx')\n",
    "mct.exporter.pytorch_export_model(quantized_model, save_model_path=onnx_path, repr_dataset=representative_data_generator)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "before proceeding to convert the model we need to make sure java 17 or up is installed. for colab you can use this dist",
   "id": "470ef3de58318d7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!sudo apt install -y openjdk-17-jre",
   "id": "fd03b5f79105d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Running the IMX500 Converter\n",
    "Now, we can convert the model to create the PackerOut which can be loaded to IMX500"
   ],
   "id": "e66a58eda61dbaae"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import subprocess\n",
    "import sys\n",
    "cmd = [\"imxconv-pt\", \"-i\", onnx_path,  \"-o\", save_folder, \"--overwrite-output\"]\n",
    "\n",
    "env_bin_path = os.path.dirname(sys.executable)\n",
    "os.environ[\"PATH\"] = f\"{env_bin_path}:{os.environ['PATH']}\"\n",
    "env = os.environ.copy()\n",
    "\n",
    "subprocess.run(cmd, env=env, check=True)"
   ],
   "id": "26051e8af246d7af",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bb7e1572",
   "metadata": {
    "id": "bb7e1572"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this tutorial, we demonstrated how to quantize a pre-trained model using MCT then convert it to a binary suitable for IMX500 execution, all with a few lines of code. for full documentation of the IMX500 converter see [here](https://developer.aitrios.sony-semicon.com/en/raspberrypi-ai-camera/documentation/imx500-converter?version=3.14.3&progLang=).\n",
    "\n",
    "## Copyrights\n",
    "\n",
    "Copyright 2025 Sony Semiconductor Israel, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
