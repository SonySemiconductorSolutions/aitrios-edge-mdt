{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Mixed-Precision Compression Post-Training Quantization + Conversion to IMX500 of a ShuffleNetV2 PyTorch Model\n",
    "\n",
    "## Overview\n",
    "This tutorial demonstrates how to use the [**Model Compression Toolkit (MCT)**](https://github.com/sony/model_optimization) for model quantization and compression under certain HW constraints (e.g. memory). This is done using MCT's mixed-precision API that assigns different bit-widths to different weights and activations in the quantized model. After the model is quantized to a specific size, it will be converted to a binary format suitable to load to IMX500 using the [**IMX500-converter**](https://developer.aitrios.sony-semicon.com/en/raspberrypi-ai-camera/documentation/imx500-converter?version=3.14.3&progLang=). \n",
    "\n",
    "This example is not intended to demonstrate evaluating MCT PTQ performance and as such intentionally uses generated random data to speed up the process.\n",
    " \n",
    "For the full tutorial on MCT's PTQ see - [*MCT Mixed-Precision PTQ PyTorch Tutorial*](https://github.com/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/pytorch/example_pytorch_mixed_precision_ptq.ipynb)\n",
    "\n",
    "For tutorials on other quantization features of MCT see [*MCT Features Tutorials*](https://github.com/sony/model_optimization/blob/main/tutorials/notebooks/mct_features_notebooks/README.md)\n",
    "\n",
    "## Summary\n",
    "In this tutorial we cover the following steps:\n",
    "\n",
    "1. Post-Training Quantization using MCT's mixed-precision API:\n",
    "   1. Use MCT to estimate the quantized model size when quantized to default single bit-width precision.\n",
    "   2. Set quantized model target size (a.k.a. Target Resource Utilization).\n",
    "   3. Quantize the model with MCT.\n",
    "2. Converting the model to a IMX500 suitable representation using IMX500-Converter\n",
    "\n",
    "## Setup\n",
    "Install the relevant packages:"
   ],
   "id": "7cf96fb4"
  },
  {
   "cell_type": "code",
   "id": "5441efd2978cea5a",
   "metadata": {},
   "source": [
    "from importlib import util, metadata\n",
    "import os\n",
    "\n",
    "if not util.find_spec('edge_mdt') or not util.find_spec(\"uni.pytorch\"):\n",
    "    print(f\"Installing edge-mdt\")\n",
    "    !pip install edge-mdt[pt]\n",
    "\n",
    "if not util.find_spec('torch') or not util.find_spec(\"torchvision\"):\n",
    "    !pip install -q torch torchvision\n",
    "\n",
    "if not util.find_spec(\"edgemdt_tpc\") or metadata.version(\"edge-mdt-tpc\") < '1.1.0':\n",
    "    raise Exception(\"Need edge-mdt-tpc>=1.1.0 for TPCv4\")\n",
    "    \n",
    "!pip install -q onnx"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "install java 17 or up so we can run the imx500-converter"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4b4ebc6a0f4dcc65"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!sudo apt install -y openjdk-17-jre"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "183c62f6c287e5bf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load a pre-trained **ShuffleNetV2** model from torchvision, in 32-bits floating-point precision format."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c2556ce8144e1d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import shufflenet_v2_x2_0, ShuffleNet_V2_X2_0_Weights\n",
    "\n",
    "float_model = shufflenet_v2_x2_0(weights=ShuffleNet_V2_X2_0_Weights.IMAGENET1K_V1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a82928d0"
  },
  {
   "cell_type": "markdown",
   "id": "c0321aad",
   "metadata": {
    "id": "c0321aad"
   },
   "source": [
    "## Representative Dataset\n",
    "We're all set to use MCT's post-training quantization. To begin, we'll define a representative dataset generator. Please note that for demonstration purposes, we will generate random data of the desired image shape instead of using real images. Then, we will apply PTQ on our model using the dataset generator we have created. For more details on using MCT, refer to the MCT tutorials"
   ]
  },
  {
   "cell_type": "code",
   "id": "618975be",
   "metadata": {
    "id": "618975be"
   },
   "source": [
    "from typing import Iterator, List\n",
    " \n",
    "NUM_ITERS = 3\n",
    "BATCH_SIZE = 2\n",
    "def get_representative_dataset(n_iter: int):\n",
    "    \"\"\"\n",
    "    This function creates a representative dataset generator. The generator yields numpy\n",
    "        arrays of batches of shape: [Batch, C, H, W].\n",
    "    Args:\n",
    "        n_iter: number of iterations for MCT to calibrate on\n",
    "    Returns:\n",
    "        A representative dataset generator\n",
    "    \"\"\"\n",
    "    def representative_dataset() -> Iterator[List]:\n",
    "        for _ in range(n_iter):\n",
    "            yield [torch.rand(BATCH_SIZE, 3, 224, 224)]\n",
    "    return representative_dataset\n",
    "representative_data_generator = get_representative_dataset(n_iter=NUM_ITERS)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Target Platform Capabilities (TPC)\n",
    "MCT optimizes the model for dedicated hardware platforms. This is done using a TPC (for more details, please visit our [documentation](https://github.com/SonySemiconductorSolutions/aitrios-edge-mdt-tpc)). Specifically for this tutorial, the IMX500 TPCv4 is used because it enables multiple bit-widths for weights (2, 4 & 8) and activations (8 & 16). **The default bit-width for single-precision is 8 bits for both weights and activations in all operations**."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "33271e23c3eff3b5"
  },
  {
   "cell_type": "code",
   "source": [
    "from edgemdt_tpc import get_target_platform_capabilities\n",
    "\n",
    "# Get a TPC object representing the imx500 hardware and use it for PyTorch model quantization in MCT.\n",
    "# Note we're using version 4.0, that supports weights & activation mixed precision required for compressing the model.\n",
    "tpc = get_target_platform_capabilities(tpc_version='4.0', device_type='imx500')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ae04779a863facd7",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d0a92bee",
   "metadata": {
    "id": "d0a92bee"
   },
   "source": [
    "## Mixed-Precision Post-Training Quantization using MCT\n",
    "Letâ€™s use MCT's resource utilization API to estimate the memory required to run the model on the IMX500. The requirement includes bith weights memory and activation memory.\n",
    "\n",
    "MCT estimates weights memory, activation memory and BOPs (bit-operations). We'll get the following values with the [Resource Utilization Data API](https://sony.github.io/model_optimization/api/api_docs/methods/pytorch_kpi_data.html#ug-pytorch-resource-utilization-data).\n",
    "\n",
    " - **Weights memory** is the static memory of the model weights.\n",
    " - **Activation memory** is the dynamic memory required by the model's activations during inference. Each step of inference of the model requires a different size of memory, depending on the current operation's input and output sizes. The *activation memory* calculated by MCT is an estimation of the **maximal activation memory** during inference.\n",
    " - **Total memory** is the sum of the *weights memory* and *activation memory*. \n",
    " - **Bit-Operations (BOPs)** is an estimation of the total multiply-accumulate operations required for a single image inference. This is a common metric for estimating latency and power requirements.\n",
    "\n",
    "The memory values above represent the estimated memory required by the model when quantized to the **default single-precision bit-width**.\n",
    "\n",
    "Note<sup>1</sup> that MCT performs this estimation on the original model and the numbers represent the **size** of weights and the **size** of the activations in the model, regardless of weights and activation types (e.g. float32 which is 4 bytes).\n",
    "\n",
    "Note<sup>2</sup> that this is only an estimation and the actual memory required by the IMX500 is larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import model_compression_toolkit as mct\n",
    "\n",
    "ru_data = mct.core.pytorch_resource_utilization_data(float_model, representative_data_generator,\n",
    "                                                     target_platform_capabilities=tpc)\n",
    "\n",
    "print('Model utilization estimation:')\n",
    "print(f' Weights:\\t\\t{int(ru_data.weights_memory)}')\n",
    "print(f' Activations:\\t{int(ru_data.activation_memory)}')\n",
    "print(f' Total memory :\\t{int(ru_data.weights_memory + ru_data.activation_memory)} bytes')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f82d414c3010d78f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The total memory estimation of the MCT is 7.86MB, so it should fit the IMX500 which has 8MB, but it's a close call. The MCT memory estimation is usually lower than the real memory used by the IMX500, because the MCT doesn't include all the memory allocations required by the IMX500 operation.\n",
    "\n",
    "Let try quantizing the model in single-precision and check that it can be converted successfully."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5c1c43407ad639ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run Converter in single precision\n",
    "\n",
    "The following section quantizes the model, exports it to ONNX, and runs the Converter."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "248292c72606613a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Quantize model:\n",
    "quantized_model, quantization_info = mct.ptq.pytorch_post_training_quantization(\n",
    "        in_module=float_model,\n",
    "        representative_data_gen=representative_data_generator,\n",
    "        target_platform_capabilities=tpc\n",
    ")\n",
    "\n",
    "# Export quantized model to an onnx file.\n",
    "save_folder = './quant_models'\n",
    "os.makedirs(save_folder, exist_ok=True)\n",
    "onnx_path = os.path.join(save_folder, 'qmodel.onnx')\n",
    "mct.exporter.pytorch_export_model(quantized_model, save_model_path=onnx_path, repr_dataset=representative_data_generator)\n",
    "\n",
    "# Run imx500-converter:\n",
    "!imxconv-pt -i ./quant_models/qmodel.onnx -o ./quant_models/output --overwrite-output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4caafe0ad0513bf4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Print the imx500-converter memory report:\n",
    "!cat ./quant_models/output/qmodel_MemoryReport.json"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eccf4776eb4ea07e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The imxconv-pt failed because there is not enough memory for the ShuffleNetV2 model quantized in single-precision (*ConvFe error (ISM) on 'qmodel': Not enough memory, available: 8.00MB , required: 8.34MB*). In the following sections, we'll compress the quantized model in mixed-precision, so it will fit the IMX500's memory limit.\n",
    "\n",
    "The MCT's memory estimation (7.86MB) is lower than the actual memory required by the IMX500 (8.34MB). In order to calculate the correct target Resource Utilization inputs to MCT, we calculate the required compression ratio: $8.00MB/8.34MB=0.96$, and multiply it by the MCT estimation.\n",
    "\n",
    "Triggering the mixed-precision in MCT requires setting a `ResourceUtilization` object with the required target memory constraints."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "36da65bf6f4a9f1e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Weights memory constraint\n",
    "\n",
    "Providing the weights constraint only, will activate mixed-precision on weights while keeping all activations at the default bit-width. In the following code, we set the compression ratio to 96%, so it will fit the IMX500 memory."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "389b0ff1d6a904d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "compression_ratio = 0.96\n",
    "\n",
    "ru = mct.core.ResourceUtilization(weights_memory=ru_data.weights_memory*compression_ratio)\n",
    "\n",
    "quantized_model, quantization_info = mct.ptq.pytorch_post_training_quantization(\n",
    "        in_module=float_model,\n",
    "        representative_data_gen=representative_data_generator,\n",
    "        target_resource_utilization=ru,\n",
    "        target_platform_capabilities=tpc\n",
    ")\n",
    "\n",
    "# Export quantized model to an onnx file.\n",
    "onnx_path = os.path.join('./quant_models/qmodel_weights_mp.onnx')\n",
    "mct.exporter.pytorch_export_model(quantized_model, save_model_path=onnx_path, repr_dataset=representative_data_generator)\n",
    "\n",
    "# Run imx500-converter:\n",
    "!imxconv-pt -i ./quant_models/qmodel_weights_mp.onnx -o ./quant_models/output --overwrite-output\n",
    "\n",
    "# Print the imx500-converter memory report:\n",
    "!cat ./quant_models/output/qmodel_weights_mp_MemoryReport.json"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2fcf3d9d3435280d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Weights and activation memory constraint\n",
    "\n",
    "The compressed quantized model accuracy may degrade compared to a non-compressed quantized model. In order to improve accuracy, MCT can exploit the fact that the activation memory requirement is the maximal activation memory during inference, and increase a certain operation activation quantization to 16 bits instead of the default 8 bits, assuming that increase doesn't change the maximal activation memory. Triggering activation memory optimization in MCT requires adding `activation_memory` to the `ResourceUtilization`.\n",
    "\n",
    "In the example below, we ask the MCT to compress weights to 96%, and keep the activation memory the same, while allowing it to set some activation bit-width to 16 bits."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6720735619a332e5"
  },
  {
   "cell_type": "code",
   "id": "63f695dd",
   "metadata": {
    "id": "63f695dd"
   },
   "source": [
    "ru = mct.core.ResourceUtilization(weights_memory=ru_data.weights_memory*compression_ratio,\n",
    "                                  activation_memory=ru_data.weights_memory)\n",
    "\n",
    "quantized_model, quantization_info = mct.ptq.pytorch_post_training_quantization(\n",
    "        in_module=float_model,\n",
    "        representative_data_gen=representative_data_generator,\n",
    "        target_resource_utilization=ru,\n",
    "        target_platform_capabilities=tpc\n",
    ")\n",
    "\n",
    "# Export quantized model to an onnx file.\n",
    "onnx_path = os.path.join('./quant_models/qmodel_weights_activation_mp.onnx')\n",
    "mct.exporter.pytorch_export_model(quantized_model, save_model_path=onnx_path, repr_dataset=representative_data_generator)\n",
    "\n",
    "# Run imx500-converter:\n",
    "!imxconv-pt -i ./quant_models/qmodel_weights_activation_mp.onnx -o ./quant_models/output --overwrite-output\n",
    "\n",
    "# Print the imx500-converter memory report:\n",
    "!cat ./quant_models/output/qmodel_weights_activation_mp_MemoryReport.json"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Total memory constraint\n",
    "\n",
    "Another option to activate weights mixed-precision and activation mixed-precision optimization is to provide a target `total_memory` to the `ResourceUtilization`. Note that using the total memory constraint allows MCT to achieve optimal memory utilization by providing a fixed total memory constraint."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7fd1bb8aa65c611"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ru = mct.core.ResourceUtilization(total_memory=ru_data.total_memory*compression_ratio)\n",
    "\n",
    "quantized_model, quantization_info = mct.ptq.pytorch_post_training_quantization(\n",
    "        in_module=float_model,\n",
    "        representative_data_gen=representative_data_generator,\n",
    "        target_resource_utilization=ru,\n",
    "        target_platform_capabilities=tpc\n",
    ")\n",
    "\n",
    "# Export quantized model to an onnx file.\n",
    "onnx_path = os.path.join('./quant_models/qmodel_total_memory_mp.onnx')\n",
    "mct.exporter.pytorch_export_model(quantized_model, save_model_path=onnx_path, repr_dataset=representative_data_generator)\n",
    "\n",
    "# Run imx500-converter:\n",
    "!imxconv-pt -i ./quant_models/qmodel_total_memory_mp.onnx -o ./quant_models/output --overwrite-output\n",
    "\n",
    "# Print the imx500-converter memory report:\n",
    "!cat ./quant_models/output/qmodel_total_memory_mp_MemoryReport.json"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7bcc5feea40ff942"
  },
  {
   "cell_type": "markdown",
   "id": "d3521637",
   "metadata": {
    "id": "d3521637"
   },
   "source": [
    "## Summary\n",
    "\n",
    "We quantized, exported and deployed the ShuffleNetV2 model in 3 different mixed-precision variations, so it will fit the IMX500 memory constraint. Each variation has its benefits, so we can choose either to fit our goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb7e1572",
   "metadata": {
    "id": "bb7e1572"
   },
   "source": [
    "## Copyrights\n",
    "\n",
    "Copyright 2025 Sony Semiconductor Israel, Inc. All rights reserved.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "you may not use this file except in compliance with the License.\n",
    "You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "Unless required by applicable law or agreed to in writing, software\n",
    "distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "See the License for the specific language governing permissions and\n",
    "limitations under the License."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
